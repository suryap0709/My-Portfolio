<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Portfolio</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
        }
        header {
            text-align: center;
            background-color: #333;
            color: #fff;
            padding: 20px;
        }
        nav {
            background-color: #444;
            padding: 10px;
            text-align: center;
        }
        nav a {
            color: #fff;
            text-decoration: none;
            margin: 0 10px;
        }
        .page-content {
            padding: 20px;
        }
        .home {
            background-image: url('Image.jpg');
            background-size: cover;
            background-position: center;
            height: 120vh;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
        }
        .home h1 {
            font-size: 3em;
            margin-bottom: 10px;
        }
        .home h2 {
            font-size: 1.5em;
            margin-top: 0;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
        }
    </style>
</head>
<body>
    <header>
        <h1>My Portfolio</h1>
    </header>
    <nav>
        <a href="#home">Home</a>
        <a href="#about">About Me</a>
        <a href="#summary">Summary</a>
        <a href="#work-history">Work History</a>
        <a href="#resume">Resume</a>
        <a href="#projects">Projects</a>
        <a href="#contact">Contact</a>
    </nav>
    
    <div id="home" class="page-content home">
        <h1>Surya Puvvada</h1>
        <h2>Data Engineer</h2>
    </div>

    <div id="about" class="page-content container">
        <h1>About Me</h1>
			<p>My name is Surya, and I'm delighted to share my journey into the world of data engineering with you.</p>
			<p>I started as a Junior Data Engineer at Netaxis IT Solutions. During my time there, I got work with Python, 
			and SQL, learning the fundamentals of data manipulation and processing.</p>
			<p>As I gained experience and confidence, I took on new challenges, eventually landing a role as a Data Engineer 
			and worked at Baxter Healthcare and Bed Bath & beyond for couple of years. This was a pivotal moment for me, as I 
			worked on big data technologies like Hadoop, Spark, and Scala and cloud platforms like GCP, AWS where I was tasked 
			with building robust data pipelines and optimizing processes for maximum efficiency.</p>
			<p>Today, I find myself at Capital Bank as a Senior Data Engineer, where I've been fortunate to lead initiatives and 
			drive innovation within the organization. My role helped me to apply my expertise in SQL query optimization, 
			Hadoop cluster management, and Python scripting to solve complex problems and deliver tangible results and I adopted 
			to azure cloud leveraging itâ€™s power to architect scalable and resilient data solutions.</p>
			<p>In summary, I bring over six years of hands-on experience in data engineering, coupled with a passion for innovation 
			and a relentless drive to succeed. I'm eager to bring my skills and expertise to a new role where I can make a meaningful 
			impact and continue pushing the boundaries of what's possible in the world of data.</p>
    </div>

    <div id="education" class="page-content container">
        <h1>Education</h1>
        <h2>University of Central Missouri</h2>
			<h3>M.S. Computer Science</h3>
			<br></br>
			<h2>Bapatla Engineering College</h2>
			<h3>B.Tech in Information Technology</h3>
    </div>
	
	<div id="skills" class="page-content container">
        <h1>Skills</h1>
        <p>Programming Languages</p>
            <ul>
                <li>Python</li>
                <li>Java</li>
				<li>SQL</li>
                <!-- Add more programming languages as needed -->
            </ul>
		<p>BigData Technologies</p>
            <ul>
                <li>Hadoop</li>
                <li>Sqoop</li>
				<li>Hive</li>
				<li>Spark</li>
				<li>Scala</li>
				<li>Kafka</li>
                <!-- Add more frameworks as needed -->
            </ul>
		<p>Cloud Platforms</p>
            <ul>
                <li>GCP</li>
                <li>AWS</li>
				<li>Azure</li>
                <!-- Add more frameworks as needed -->
            </ul>
		<p>ETL Tools</p>
            <ul>
                <li>Apache Airflow</li>
                <li>Apache Spark</li>
				<li>Apache Kafka</li>
                <!-- Add more frameworks as needed -->
            </ul>
		<p>Databases</p>
            <ul>
                <li>MS SQL Server</li>
                <li>Teradata</li>
				<li>Impala</li>
				<li>MongoDB</li>
                <!-- Add more frameworks as needed -->
            </ul>
		 <p>Datawarehouse</p>
            <ul>
                <li>Snowflake</li>
                <!-- Add more frameworks as needed -->
            </ul>
		<p>Data Integration</p>
            <ul>
                <li>Informatica</li>
                <!-- Add more frameworks as needed -->
            </ul>
		<p>Data Visualization</p>
            <ul>
                <li>Tableau</li>
                <li>Power BI</li>
                <!-- Add more frameworks as needed -->
            </ul>
    </div>

    <div id="work-history" class="page-content container">
        <h2>Work History</h2>
        <h3>Capital Bank</h3>
            <p>Role: Sr. Data Engineer</p>
            <p>From: November 2022 - To: Present</p>
		<h3>Bed Bath & Beyond</h3>
            <p>Role: Data Engineer</p>
            <p>From: April 2020 - To: August 2022</p>
		<h3>Baxter Healthcare</h3>
            <p>Role: Data Engineer</p>
            <p>From: January 2019 - To: March 2020</p>
			<h3>Netaxis IT Solutions</h3>
            <p>Role: Jr.Data Engineer</p>
            <p>From: November 2017 - To: December 2018</p>
    </div>

    <div id="projects" class="page-content container">
        <h2>Projects</h2>
        <h1>Enhanced Data Engineering Pipeline for Efficient Cloud-Based Data Management</h1>
            <h2>Abstract:</h2>
			<p>This project aims to address these challenges by developing a robust data engineering pipeline leveraging cutting-edge technologies 
			such as Apache Spark, Apache Sqoop, and Azure Cloud. </p>
            <h2>Motive:</h2> 
			<p>The motive behind this project is to revolutionize the data engineering landscape by implementing advanced techniques and tools to 
			streamline data management processes.</p>
            <h2>Requirements:</h2>
       		<p>Python, Apache Spark, Apache Sqoop, Azure Cloud, SQL, MongoDB, Tableau, Snowflake</p>
            <h2>Explanation:</h2>
			<p>The project begins with data extraction from various sources using Apache Sqoop, ensuring seamless integration with the Azure Cloud platform.</p>
			<p>Scalable ETL processes are engineered and maintained using Apache Spark to handle large volumes of data, ensuring data consistency and accuracy throughout the entire workflow.</p>
			<p>Collaboration with cross-functional teams enables the integration of data visualization solutions, utilizing Tableau for creating insightful dashboards based on processed data from Apache Kafka streams.</p>
			<p>Routine health checks are conducted on Kafka clusters to ensure data consistency and availability, proactively addressing any potential issues. Data quality checks within SQL transformations are enhanced, implementing error handling mechanisms to maintain the integrity of processed data in Snowflake.</p>
            <h2>Results:</h2> 
			<p>The integration of cutting-edge technologies such as Apache Spark, Apache Sqoop, and Azure Cloud ensures scalability, efficiency, and reliability.</p>
			<p>The project enhances data discoverability, accessibility, and integrity, empowering organizations to derive valuable insights from their data assets efficiently and effectively.</p> <br><br/>
			
			<h1>Cloud-based Data Integration and Analytics Platform: Empowering Enterprises with Scalable Insights</h1>
            <h2>Abstract:</h2>
			<p>In today's data-driven landscape, enterprises encounter the challenge of effectively managing and analyzing vast volumes of data across diverse sources. 
			To address this challenge, a comprehensive cloud-based data integration and analytics platform have been developed.</p>
            <h2>Motive:</h2> 
			<p>By implementing a comprehensive data integration and analytics platform, this project aims to empower enterprises to harness the full potential of 
			their data assets, drive innovation, and gain a competitive edge in the market.</p>
            <h2>Requirements:</h2>
       		<p>AWS services, Informatica, Hadoop, Apache Spark, Kafka, Teradata, Power BI, Apache Airflow, SQL </p>
            <h2>Explanation:</h2>
			<p>Leveraging AWS services as the foundation, the platform seamlessly integrates data from various sources including Teradata, MongoDB, and SQL Server.</p>
			<p>Informatica is utilized for data integration tasks, ensuring smooth data movement and implementing transformations for data cleansing and enrichment.</p>
			<p>Real-time data streaming pipelines are established with Apache Kafka, enabling seamless communication between disparate systems and facilitating timely data delivery for analytics.</p>
			<p>The platform also incorporates advanced data security measures, including encryption and access controls, to ensure the confidentiality and integrity of sensitive data stored in Hadoop and AWS environments.</p>
            <h2>Results:</h2> 
			<p>The establishment of real-time data streaming pipelines with Kafka enables timely data delivery for analytics, empowering enterprises to make informed decisions.</p>
			<p>Interactive dashboards created using Power BI provide stakeholders with real-time visibility into key performance indicators, enabling informed decision-making.</p><br></br>
			
			<h1>Enterprise Data Optimization and Business Intelligence Framework</h1>
            <h2>Abstract:</h2>
			<p>The Enterprise Data Optimization and Business Intelligence Framework (EDOBI) is a comprehensive solution designed to address the challenges faced by modern corporations in efficiently managing and deriving insights from vast volumes of data.
			This project aims to empower corporations with actionable insights for strategic decision-making, driving operational efficiency and competitive advantage.</p>
            <h2>Motive:</h2> 
			<p>The primary motive behind EDOBI is to provide corporations with a comprehensive framework that streamlines data processing, enhances data quality, and enables sophisticated analysis to uncover actionable insights. By optimizing data management and 
			leveraging advanced analytics, EDOBI empowers corporations to make informed decisions and stay ahead in their respective industries.</p>
            <h2>Requirements:</h2>
       		<p>Python, SQL, Hive, Map Reduce, GCP, Informatica, Hadoop, Teradata, MS SQL Server, HiveQL</p>
            <h2>Explanation:</h2>
			<p>Data pipelines are configured and managed on GCP to ensure scalability, reliability, and security. Integration with services like Big Query facilitates analytical processing and real-time insights generation.</p>
			<p>EDOBI facilitates in-depth data analysis using SQL queries, focusing on data profiling, statistical analysis, and predictive modeling to uncover valuable insights and trends.</p>
			<p>Complex data retrieval and manipulation tasks are performed using optimized SQL queries, tailored to Teradata and MS SQL Server environments to enhance query performance.</p>
			<p>Python scripts are developed to extract, transform, and load (ETL) data efficiently, ensuring data integrity and consistency across the organization.</p>
            <h2>Results:</h2> 
			<p>EDOBI's scalable architecture allows corporations to adapt to evolving data requirements and business needs, ensuring continued relevance and effectiveness.</p>
			<p>EDOBI's robust data governance measures ensure compliance with regulatory requirements and protect sensitive corporate information, enhancing trust and mitigating risk.</p><br></br>
			
			<h1>Integrated Data Engineering Project: Streamlining Operations with SQL-Hadoop Fusion</h1>
            <h2>Abstract:</h2>
			<p>This project focuses on implementing a comprehensive data engineering solution, leveraging SQL and Hadoop technologies, to optimize data processing workflows, 
			enhance database performance, and enable seamless integration of diverse data sources.</p>
            <h2>Motive:</h2> 
			<p>Organizations often struggle with disparate datasets stored across different systems, leading to inefficiencies and delays in deriving insights. The primary objective of this project is to address 
			these challenges by designing and implementing a robust data engineering solution that harmonizes SQL and Hadoop technologies. </p>
            <h2>Requirements:</h2>
       		<p>SQL, MS SQL Server, Hadoop, HDFS, Map Reduce, Scripting, ETL</p>
            <h2>Explanation:</h2>
			<p>The project begins with a comprehensive assessment of the organization's data infrastructure and processing workflows.</p>
			<p>Advanced SQL query optimization techniques are applied to enhance the performance and responsiveness of MS SQL Server databases, focusing on data normalization, indexing, and query rewriting.</p>
			<p>Apache Hive is utilized for querying and analyzing data stored in Hadoop clusters, providing a powerful tool for extracting valuable insights.</p>
			<p>Additionally, agile methodologies are employed to adapt data engineering workflows to evolving business requirements, ensuring flexibility and maintainability.</p>
            <h2>Results:</h2> 
			<p>Enhanced database performance and responsiveness through optimized SQL queries and indexing strategies.</p>
			<p>Streamlined data transformation processes, automated through Python scripts, leading to improved operational efficiency.</p>
			
    </div>

    <div id="contact" class="page-content container">
	<h2>Contact</h2>
            <!-- Content for contact page -->
            <form action="mailto:suryap0709@example.com" method="post" enctype="text/plain">
                <label for="fname">First Name:</label><br>
                <input type="text" id="fname" name="fname"><br>
                <label for="lname">Last Name:</label><br>
                <input type="text" id="lname" name="lname"><br>
                <label for="message">Message:</label><br>
            </form>
            <button onclick="sendMessage()">Send Message</button>
    </div>
	    <script>
        function sendMessage() {
            alert("Message sent successfully!");
        }
    </script>

</body>
</html>
